{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ERC Research Notes","text":"<p>Welcome!</p> <p>This is a compilation of research paper notes and summaries of other important resources by the Electronics and Robotics Club, BITS Goa.</p>"},{"location":"about/","title":"About Us","text":"<p>The Electronics and Robotics Club (ERC) of BITS Goa is a diverse group of students with interests ranging from electronics to machine learning to mechanical design. Over the years, we have evolved into a platform to learn and experiment with various aspects of science and engineering and to apply them in robotics. We are open to anyone with a general interest in engineering and who wants to explore robotics, so feel free to get in touch with us.</p> <p>This is a compilation of notes based on research papers and other related resources that we have gone through, making it easier for anyone who wants to refer to these resources, providing them with an introductory, yet detailed summary on what is covered in these papers.</p> <p>You can find out more about us on our website and blog.</p>"},{"location":"blank/","title":"No content yet","text":"<p>Check the contributions page to add content.</p>"},{"location":"controls/beeclust/","title":"BEECLUST: A Swarm Algorithm Derived from Honeybees.","text":"<p>Summarised By: Arjun Puthli </p> <p>Refer to paper here</p>"},{"location":"controls/beeclust/#biological-inspiration","title":"Biological Inspiration","text":"<p>The inspiration for this algorithm was how bees behave in environments with temperature gradients. Consider a pipe with one end being heated while the other is in a cold water bath. This creates a steep temperature gradient. Bees tend to aggregate towards warmer areas and group up in some optimum spot. Certain rules were created to model this behaviour. Due to sensor limitations on the bot, this environment was replicated using a light source.</p>"},{"location":"controls/beeclust/#algorithm-summary","title":"Algorithm Summary","text":"<p>The algorithm has a few general steps that it follows:</p> <ol> <li>The bot moves in some random direction</li> <li>If the bot encounters a wall, it moves away. We can model this behaviour in two ways<ul> <li>bot turns around and goes back the way it came or</li> <li>bot chooses a new random point and goes there</li> </ul> </li> <li>If the bot encounters another robot, they stop in the same spot for a certain amount of time T. After time T has elapsed, the bots behave like they would in step 2.</li> </ol> <p>The waiting time depends upon the brightness of the spot the bots are currently in. By setting some maximum waiting time, T can be calculated by the following formula:</p> \\[ T(e) = \\frac{T_{max}e^{2}}{e^{2}+7000} \\] <p>Where e is the sensor reading scaled linearly between 0 to 180, corresponding to the range of 0 lux to 1500 lux (The sensor is the one used on the Jasmine bot, hardware details can be found here www.swarmrobot.org).</p>"},{"location":"controls/beeclust/#compartment-model","title":"Compartment Model","text":"<p>Consider an arena with concentric circle zones, each with different light intensities. The zone right in the centre has the brightest intensity, and it gets dimmer as you go radially outwards. </p> <p></p> <p>Robot Diffusion: Since robots move from one zone to another, the rate at which they flow can be represented by \\(\\delta_{ij}\\) where the robots are moving from zone i to zone j.</p> <p>Change of State:  When a robot encounters another robot the robot stops for a certain amount of time. The rate at which this change of state occurs is represented by \\(\\alpha_i\\). The bots start moving again after a certain amount of time elapses, and the rate at which this occurs is represented by \\(\\beta_i\\)</p> <p>The rate at which robots move from zone I to I-1 can be written as:</p> \\[ \\delta_{i,i-1}(t) = 0.5vDF_i(t)\\frac{R_{i-1}}{(R_i + R_{i-1})(R_i - R_{i-1})} \\] <p>Where \\(F_i(t)\\) is the number of free roaming robots in the zone, D = 0.5 is the diffusion coefficient and v is the average velocity of the bots. Since the outermost zone has only a single neighbour and is shaped slightly differently, the diffusion of bots from zone 5 to zone 4 can be modelled as:</p> \\[ \\delta_{5,4}(t) = \\frac{vDF_5(t)}{l_{arena}- R_4} \\] <p>The robots also have a certain likelihood of recognising another bot as the arena wall, or another robot. Let \\(P_{detect}\\) be the probability that a bot recognizes another bot as a bot and not a wall, and let \\(z_i\\) represent the area of the focal zone \\(i\\). \\(C_{f,f}\\)  is the likelihood of two free moving robots colliding with each other, while \\(C_{f,a}\\)   is the likelihood of a freemoving robot coliding with a stationary robot. The rate at which the robots collide and aggregate together can be modelled as:</p> \\[ \\alpha_i(t) = F_it(t)P_{detect}\\frac{C_{f,a}A_i(t) + C_{f,f}F_i(t)}{Z_i} \\] <p>The rate at which stationary robots start to move again is represented by \\(\\beta_i\\) for each zone \\(i\\). The waiting time can be approximated by \\(\\frac{1}{w_i}\\) where \\(w_i\\) is the average waiting period of the robots in zone \\(i\\).</p> <p>According to the Jasmine bot\u2019s hardware, the luminance sensor maps \\(E_i\\) between 0 and \\(l_{max} =\\)  1500 lux in a linear manner. For this range of luminance, the sensor reports a value between 0 and 180, therefore:</p> \\[ e_i = min(180,256\\frac{E_i}{l_{max}}) \\] <p>The average waiting time modelled on the sensor output is (where \\(w_{max}\\) is the maximum waiting time):</p> \\[ w_i = max(1, \\frac{w_{max}e_i^{2}}{e_i^{2}+7000})  \\] <p>If \\(A_i(t)\\)  is the number of robots aggregated together in a zone, then the rate at which stationary bots start moving again is:</p> \\[ \\beta_i(t) = \\frac{A_i(t)}{w_i} \\] <p>The above equations can be merged into an ODE to represent the rate of change of free moving robots in a zone \\(i\\):</p> \\[ \\frac{dF_i(t)}{dt} = \\delta_{i-1, i}(t) + \\delta_{i+1, i}(t) - \\delta_{i, i-1}(t) - \\delta_{i, i+1}(t) - \\alpha_i(t) + \\beta_i(t) \\] <p>For the right-most compartment \\(i=1\\) we set \\(\\forall\\) t   \\(\\delta_{i-1,i}(t) = \\delta_{i,i-1}(t) = 0\\)</p> <p>The number of aggregated robots in each zone is:</p> \\[ \\frac{dA_i(t)}{dt} = \\alpha_i(t) - \\beta_i(t) \\]"},{"location":"controls/beeclust/#macroscopic-model","title":"Macroscopic Model","text":"<p>In this approach, instead of taking separate compartments/zones, continuous space is considered by using partial differential equations. The approach considered by the authors was making use of Brownian motion. The change in a particle\u2019s position X can be described by Langevin\u2019s equation under certain assumptions:</p> \\[ \\frac{dX}{dt} = -Q + DW \\] <p>Where Q is the drift, D is the diffusion and W is a stochastic process. Using the Kolmogorov forward equation (\\(P'(t) = P(t)Q\\), where P(t) is the probability matrix and Q is the generator).</p> <p>The Kolmogorov forward equation corresponding to the above equation is:</p> \\[ \\frac{\\partial \\rho(x,t)}{\\partial t} = -\\nabla(Q\\rho(x,t)) + \\frac{1}{2}\\nabla^{2}(D^{2}\\rho(x,t)) \\] <p>\\(\\rho(x,t)d_{x}d_{y}\\)  is the probability of encountering the particle at point x within a rectangle defined by dx and dy at time t. The above equation can be simplified by only considering diffusion:</p> \\[ \\frac{\\partial \\rho(x,t)}{\\partial t} =  \\frac{1}{2}\\nabla^{2}(D^{2}\\rho(x,t)) \\]"},{"location":"controls/probab_aggregation/","title":"Probabalistic Aggregation Strategies in Swarm Robotic Systems","text":"<p>Summarised By: Arjun Puthli </p> <p>Refer to paper here</p>"},{"location":"controls/probab_aggregation/#about-the-paper","title":"About the paper","text":"<p>This paper focuses on modelling individual, local robot behaviours such that they are transformed into a globally optimum behaviour. In this case, the focus being on controlling the aggregation and clustering behaviour of a swarm of robots making use of a simple probabalistic finite state machine.</p>"},{"location":"controls/probab_aggregation/#algorithm-summary","title":"Algorithm Summary","text":"<p>The controller consists of two layers: A higher level consisting of the robot approach, repel and waiting behaviours and a lower level obstacle avoidance behaviour. The lower level obstacle avoidance behaviour depends on infra-red sensor information, and overall doesn\u2019t contribute to the swarm aggregation. Therefore the focus is on the finite state machine of the higher level, which works as follows:</p> <p>As mentioned earlier the higher level consists of approach, repel and wait behaviours. They can be described as:</p> <ul> <li> <p>Approach: The robot uses sound sensors to estimate the relative direction of the loudest noise source and drives in that direction.</p> </li> <li> <p>Wait: The robot stays in place</p> </li> <li> <p>Repel: Acts opposite to approach, and drives the robot in the opposite direction of the loudest sound source.</p> </li> </ul> <p>Every robot starts out in the approach state. It continues to be in this state until it is able to detect at least one other robot (with infrared sensors), and then switches to the wait state. This condition is termed as the \u201crobot close\u201d condition in the paper. </p> <p>Consider fixed values \\(P_{return}\\) and \\(P_{leave}\\) \\(\\epsilon\\) \\([0,1]\\)</p> <p>During the wait state the robot samples a value \\(P\\) uniformly as $U \\sim [0,1] $. If \\(P &lt; P_{leave}\\) then the robot moves to the repel state. Else it continues in the wait state.</p> <p>In the repel state, if \\(P &lt; P_{return}\\), then robot moves to the approach state, else it continues in the repel state.</p>"},{"location":"controls/probab_aggregation/#performance-metrics","title":"Performance Metrics","text":"<ol> <li> <p>Expect Cluster Size (ECS): Makes use of a threshold value \\(T_{RobotClose}\\), such that if the distance between any two robots \\(d\\) is less than \\(T_{RobotClose}\\), they are considered neighbours. The cluster size for a robot \\(R_i\\) is defined as \\(Size({R_i})\\), which is the number of robots in the cluster that robot belongs to. Hence ECS can be defined as follows: \\(\\(ECS = \\frac{1}{n}\\sum_{i=1}^{n}{Size^2({R_i})}\\)\\)  This metric ignores spatial distribution of the clusters, but is a measure of the size of the clusters.</p> </li> <li> <p>Total Distance:  This provides more information regarding spatial configuration of the swarm and the clusters. It uses negative distance to emphasize better clustering for lower distance values. The metric is defined as follows: \\(\\(TD = -\\sum_{i=1}^n \\sum_{j=i+1}^n dist(R_i, R_j)\\)\\)</p> </li> </ol>"},{"location":"controls/probab_aggregation/#effects-of-changing-the-parameters","title":"Effects of changing the parameters","text":"<ol> <li> <p>\\(P_{leave} \\neq 0, P_{return} \\neq 0\\): Increasing \\(P_{leave}\\) reduces the size of the clusters and increases the number of robots searching for a cluster. On the other hand increasing \\(P_{return}\\) increases the cluster size. </p> </li> <li> <p>\\(P_{leave} = 0\\): In this strategy, when a robot nears another robot, it stops and waits there forever since the probability of switching to the repel state is 0. This leads to frozen aggregation, and expected cluster size is independent of \\(P_{return}\\).</p> </li> <li> <p>\\(P_{return} = 0\\): All robots move to the repel state and no clustering occurs.</p> </li> <li> <p>\\(P_{return} = P_{leave} = 1\\): Similar to strategy 1, but the robots neve enter the wait state. </p> </li> </ol>"},{"location":"maths/about/","title":"About this Section","text":"<p>This section covers essential mathematical concepts and theorems that one might find in most research papers ranging from measure theory and linear algebra concepts like probability measures and vector spaces, to markov chains.</p> <p>Feel free to proof read and make modifications, or add contributions of your own.</p>"},{"location":"maths/banach/","title":"Banach\u2019s Fixed Point Theorem","text":"<p>Theorem:  The theorem also known as the contraction mapping theorem, states that if a function \\(\\tau\\) is a contraction mapping on a complete metric space \\((X,d)\\), then \\(\\tau\\) has a unique fixed point in \\(X\\).</p> <p>We approach the proof by first defining the contraction mapping, and then using an iterative method to show the existence of a fixed point.</p>"},{"location":"maths/banach/#some-preliminaries","title":"Some Preliminaries","text":"<p>Contraction:  A function \\(\\tau: X \\mapsto X\\) is a contraction mapping on a metric space \\((X,d)\\) if there exists a constant \\(0 &lt; k &lt; 1\\) such that \\(\\forall\\) \\(x,y \\in X\\):</p> \\[ d(\\tau(x),\\tau(y)) \\leq kd(x,y) \\] <p>Complete Metric Space:  A metric space \\((X,d)\\) is complete if every Cauchy sequence in \\(X\\) converges to a point in \\(X\\).</p> <p>Cauchy Sequence:  A sequence \\((x_n)\\) in a metric space \\((X,d)\\) is a Cauchy sequence if \\(\\forall\\) \\(\\epsilon &gt; 0\\), \\(\\exists\\) \\(N \\in \\mathbb{N}\\) such that \\(\\forall\\) \\(m,n \\geq N\\):</p> \\[ d(x_m,x_n) &lt; \\epsilon \\]"},{"location":"maths/banach/#proof","title":"Proof","text":"<p>Let \\((X,d)\\) be a complete metric space and \\(\\tau: X \\mapsto X\\) be a contraction mapping on \\(X\\). We will show that \\(\\tau\\) has a unique fixed point in \\(X\\) ie. \\(\\exists\\) \\(x^* \\in X\\) such that \\(\\tau(x^*) = x^*\\).</p> <p>We can define an iterative process as follows:</p> <p>$ x^{(0)} \\in X \\ x^{(n+1)} = \\tau(x^{(n)}) \\quad \\forall n \\in \\mathbb{N}\\ $</p> <p>Hence we obtain  asequence {\\(x^{(n)}\\)} in \\(X\\). We need to show that this sequence converges. Since the sequence is in a complete metric space, it is enough to show that this sequence is Cauchy.</p> \\[ \\begin{aligned} d(x^{n}, x^{n+1}) &amp;= d(\\tau(x^{(n-1)}), \\tau(x^{(n)}))\\\\      &amp;\\leq kd(x^{(n-1)}, x^{(n)})\\\\      &amp;\\leq k^2d(x^{(n-2)}, x^{(n-1)})\\\\      &amp;\\vdots\\\\      &amp;\\leq k^nd(x^{(0)}, x^{(1)})\\\\ \\end{aligned} \\] <p>Now by the triangular inequality we can see that:</p> \\[ \\begin{aligned} d(x^{(m)}, x^{(n)}) &amp;\\leq d(x^{(m)}, x^{(m+1)}) + d(x^{(m+1)}, x^{(m+2)}) + \\dots + d(x^{(n-1)}, x^{(n)})\\\\      &amp;\\leq k^md(x^{(0)}, x^{(1)}) + k^{m+1}d(x^{(0)}, x^{(1)}) + \\dots + k^{n-1}d(x^{(0)}, x^{(1)})\\\\      &amp;= k^md(x^{(0)}, x^{(1)})\\sum_{i=0}^{n-m-1}k^i\\\\      &amp;= k^md(x^{(0)}, x^{(1)})\\frac{1-k^{n-m}}{1-k}\\\\      &amp;\\leq \\frac{k^md(x^{(0)}, x^{(1)})}{1-k}\\\\ \\end{aligned} \\] <p>Since \\(0 &lt; K &lt; 1\\) we can choose \\(m\\) sufficiently large such that \\(d(x^{m}, x^{n})\\) is small, and hence the sequence is Cauchy.</p> <p>Finally, since the sequence is Cauchy, it converges to a point \\(x \\in X\\), ie. \u00a0 \\(x^{(n)} \\xrightarrow{} x\\)</p> <p>We now need to show that this point is a fixed point.</p> \\[ \\begin{aligned} d(x,\\tau(x)) &amp;\\leq d(x,x^{(n)}) + d(x^{(n)}, \\tau(x))\\\\ &amp;= d(x,x^{(n)}) + d(\\tau(x^{(n-1)}), \\tau(x))\\\\ &amp;\\leq d(x,x^{(n)}) + Kd(x^{(n-1)}, x)\\\\ \\end{aligned} \\] <p>Since the sequence converges to \\(x\\), \u00a0 \\(d(x,\\tau(x)) \\leq 0\\) and hence \\(x = \\tau(x)\\).</p> <p>Finally, we need to show that this fixed point is unique. Suppose there exists another fixed point \\(y \\in X\\) such that \\(y = \\tau(y)\\). Then:</p> \\[ \\begin{aligned} d(x,y) &amp;= d(\\tau(x), \\tau(y))\\\\ &amp;\\leq Kd(x,y)\\\\ \\end{aligned} \\] <p>This implies \\(d(x,y) = 0\\) and hence \\(x = y\\) since \\(0 &lt; K &lt; 1\\)</p> <p>Hence we have shown that the sequence of approximations converges to a point that is the fixed point, and that this fixed point is unique.</p> <p>This completes the proof.</p>"}]}